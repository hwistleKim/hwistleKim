---
layout: single
title: Discord 한 메세지를 동시에 삭제와 수정을 한다면?
categories: database
tags:
  - discord
  - MongoDB
  - Cassandra
toc: "true"
author_profile: false
sidebar:
  nav: docs
---
[원문:How Discord Stores Trillions Of Messages](https://discord.com/blog/how-discord-stores-billions-of-messages)

정답부터 말하면 이 공식 블로그 포스팅에 따르면 <span style="background-color: yellow; color: white;">삭제가 </span>된다고 한다. 

Discord는 기존에 MongoDB를 사용하여 사용자들의 메세지를 저장했다. 2015년 11월쯤 1억개의 메세지가 저장되고 나서는 더 이상 인덱스가 서버의 Ram 크기와 맞지 않게 되었고 이로 인해 메모리 페이지 교체로 인한 응답 지연이 발생할 확률이 올랐다.

그래서 특정 조건들을 서버의 크기와 활성도에 따라서 구분하고 만들었고 이를 만족 시키는 건 Cassandra 밖에 없었다고 한다.

Cassandra의 특징을 가장 잘 설명하는건 KKV(Key Key Value) 방식이다. MongoDB에서 cahnnel_id, created_at 두개를 p-key로 사용했던 것을 Snowflake 방식을 사용하며 KKV인 (channel_id,bucket),message_id 로 바꿨다.

여기서 흥미로운 점이 bucket 의 크기를 설정하는 일이었는데 Cassandra 는 2G 의 partition 을 지원 한다고 했지만 실제 사용시 GC 의 문제로 100MB가 넘어가면 warning 이 발생했고 따라서 가장 큰 discord 채널의 10일 간의 메세지의 양으로 bucket(partition) 의 크기를 결정하면 100MB 가 맞았다고 한다.

AC 위주 방식인 Cassandra 에서 consistency를 지원하기 위해서 여러 사용자가 한 메세지에 대해 삭제와 변경을 동시에 할 경우 삭제를 하는 로직으로 해결 보았다고 한다. (필드 검색을 했을때 null값이 생긴걸로 이 문제가 발생한다는 걸 먼저 인지했다고)

이때 쓰기 작업에서 비효율도 발견했는데 Cassandra 의 특성상 데이터를 즉시 삭제하지 못하고 다른 노드로 복사가 되며 삭제 작업을 "Tombstone" 이라는 쓰기 형태로 처리하고, "Tombsotne" 은 10일(default)이 유지된다고 한다.(이는 후술할 또 다른 문제를 만들었다.)
Cassandra 의 또 다른 특성으로 모든 쓰기 작업은 upsert(update + insert)이기 때문에 처음으로 null 을 작성할때도 "Tombstone" 이 생성된다고 한다.
Discord 의 전체 메시지 스키마에는 16개의 열이 있지만 평균적인 메세지의 사용은 4개였고 이를 해결하기 위해 non-null 값만 사용하도록 바꾸었다고 한다.

당연히 기대한만큼 성능이 나왔고 좋았는데 문제가 생긴다.

이 글을 읽으며 가장 신기했던 내용인데 
6개월간 잘 운영하다가 Cassandra가 주기적으로 10초간 "Stop-the-world" GC 문제를 발견했다고 한다. 범인을 찾고 보니 한 Discord 공식 채널에서 로딩에만 20초가 걸리는 채널을 찾았고 놀랍게도 메세지가 한개만 남아있었다고 한다. 수만개의 메세지를 discord API 를 이용해 지웠고 이는 수만개의 tombstone 을 생성했으며 이는 JVM 의 GC 속도보다 빨랐다고

유지기간 10일을 2일로 줄이고 query 코드를 빈 bucket 을 회피하도록, 즉 최악의 경우에도 현존하는 메시지가 들어있는 하나의 bucket만 스캔하도록 고쳐 문제를 해결했다.

2017년 이 글 작성 당시에는 12개의 node cluster 와 3개의 replica 가 있지만 나중에는 늘릴 가능성이 있다고, 그리고 Cassandra 새로운 버전을 사용해서 두배가량 bumping 할 계획이었고, 장기로는 Scylla 를 이용해 anti-antropy process 시간을 상당히 줄이고 구글 클라우드 스토어에 자주 사용 안되는 데이터를 저장하는 방식도 생각 중인데 후자는 안 할 가능성이 높다고 한다.

결론은 DB 바꿨어도 순항중이고 이때만 하더라도 백엔드 개발자가 4명이라 사람을 더 구한다는 내용이었다.(헬다이버즈2도 서버 4명이서 갈렸다고 하던데....) 23년도에 이 글의 후속편이 올라왔는데 그 포스팅도 읽어볼 예정이다.


